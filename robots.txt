# This robots.txt file controls crawling of URLs under https://example.com.
# All crawlers are disallowed to crawl files in the "includes" directory, such
# as .css, .js, but Googlebot needs them for rendering, so Googlebot is allowed
# to crawl them.
User-agent: *
Disallow: /wedding/
disallow: /wedding/
Disallow: /wedding/index.html
disallow: /wedding/index.html
Disallow: /wedding/rsvp.html
disallow: /wedding/rsvp.html
Disallow: /wedding_invitation/
disallow: /wedding_invitation/
Disallow: /wedding_invitation/index.html
disallow: /wedding_invitation/index.html
Disallow: /wedding_invitation/rsvp.html
disallow: /wedding_invitation/rsvp.html
